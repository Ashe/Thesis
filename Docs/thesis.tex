\documentclass[10pt]{article}
\usepackage[left=40mm, right=20mm, top=20mm, bottom=20mm, includefoot]{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[style=apa,backend=biber, language=english]{biblatex}
\usepackage{url}
\addbibresource{sources.bib}
\setlength{\parindent}{0ex}
\setlength{\parskip}{1.5ex}
\font\titlefont=cmr10 at 18pt

\lstset{
  frame=tb, % draw a frame at the top and bottom of the code block
  tabsize=4, % tab space width
  showstringspaces=false, % don't mark spaces in strings
  numbers=left, % display line numbers on the left
  commentstyle=\color{green}, % comment color
  keywordstyle=\color{blue}, % keyword color
  stringstyle=\color{red} % string color
}

\begin{document}

\title{\titlefont The path to the right decision: An investigation into using heuristic pathfinding algorithms for decision making}
\author{Ashley Smith}
\date{\today}
\maketitle

\renewcommand\abstractname{\textbf{Abstract}}
\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras justo velit, vestibulum sit amet turpis in, interdum rhoncus magna. Proin pulvinar posuere iaculis. Duis vulputate tristique arcu, id pretium ante blandit ut. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Nam augue tellus, mattis quis consequat id, facilisis eu lectus. Vivamus euismod non quam sed condimentum. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Phasellus vitae consequat nisi. Morbi vulputate tellus ut nibh vulputate, vitae blandit ex faucibus.
\end{abstract}

\clearpage
\section{Introduction}

\subsubsection{Video games and artificial intelligence}

Games are good for the global economy. \citeauthor{Newzoo} reports that in October 2019 the global games market was worth \$148 billion with mobile games accounting for 46\% of that. In order to remain competetive in the growing and changing market, developers are pushed to make bigger, better and more complex games. Blow (\citeyear{blow2004game}) describes how the evolution of the technologies available allowed for a greater number of elements to be simulated in the game world at the same time, which in turn could increase a game's overall potential worth. Video games are just as much about their design as they are about their programming, and the entry barrier to creating good looking and well executed games has been lifted with the establishment of engines like Unity (\cite{Unity}) and Unreal (\cite{Unreal}) to the point where even non-programmers can get involved using textual or visual scripting.

For many years, making a good game has been about improving graphics to be higher quality and more realistic (\cite{yap2002grid}; \cite{blow2004game}). However, it is becoming clear that graphics aren't everything and that good physics systems or competent AI (Artificial Intelligence) can improve the end-user's experience just as much as graphics if not more (\cite{blow2004game}). When referring to AI in a games context, there is a significant difference between the approaches used to create academic AI and the AI used in games. The average game AI simulates a decision making process where the actions to be taken are as believable and fun as possible in the game's context whereas the objective of an academic AI is typically to achieve a level of intelligence or autonomy that enables the AI to optimally excel at a given task (\cite{nareyek2004ai}, p.60). It's not difficult to make a simply AI that always wins against the player, but player's wouldn't enjoy a game they couldn't win (\cite{tozour2002evolution}), and so the distinction between the objectives of these approaches is very important.

\subsubsection{The need for game AI}

AI has multiple uses within a games context but the majority of use cases employ AI to control the characters featured in a level of the game. \citeauthor{laird2001human} (\citeyear{laird2001human}, p.16) said that whether these characters are replacements for opposing players (nicknamed 'Bots') or used to create NPCs (Non Player Characters) to act as companions, villains and plot devices in a role-playing or narrative context, "human-level AI can expand the types of experiences people have playing computer games". Using AI opens up the opportunity for increasing the difficulty of the game, and, so long as the AI can be adapted to the player's strength, this difficulty will be perceived as the same kind of challenges that make games fun (\cite{buro2004call}, p.2). \citeauthor{laird2001human} (\citeyear{laird2001human}, p.16) also hypothesised that utilising such an AI is a good step towards the development of enjoyable and challenging gameplay, and potentially to "completely new genres" (\cite{laird2001human}, p.17). While "customers value great AI" (\cite{nareyek2004ai}, p.60), the fact that marketing material cannot advertise enjoyable game AI as easily as appealing graphics means that, in comparison to graphics or physics, AI rarely gets the same time or resource investment it needs to improve (\cite{nareyek2004ai}, p.60).

When developing game AI, it is very important to choose a suitable approach that fulfills the requirements of the game (\cite{millington2019ai}, p.19). Academic AI can be made using algorithms inspired by biology such as neural networks or genetic algorithms and trained through iteratation or with datasets. Genetic algorithms simulate natural selection and evolution using data to immitate gene crossover and mutation (\cite{tozour2002evolution}). Similarly, neural networks simulate neuron patterns in the brain to map inputs to outputs (\cite{tozour2002evolution}). These approaches aren't used in game AI due to the requirements necessary to train, tune and test the AI, moreover, the AI would need to be tailored to play a given game in order to take advantage of game specific requirements (\cite{nareyek2004ai}, p.64). Instead, game AI developers embrace simpler, non-learning algorithms due to them being easier to understand, implement and debug (\cite{tozour2002evolution}, p.7).

\subsubsection{AI development}

The most basic forms of AI used in games take the form of a series of if-then statements and are known as a 'production rule systems' (\cite{tozour2002evolution}). The AI is divided into two parts: the first being the ruleset that the AI has to follow and the second is the execution of the response to said rules. The result is a very basic AI that is not only limited to what actions it can take but also when it can take them. AI created in this way is perfectly suited to simple games with a small number of scenarios that the programmer can check for and respond to by hand. Additionally, games where there are a lot of agents needing to make decisions simultaneously would also benefit from a basic AI.

Decision trees are another way to create AI that have applications for games. Building on the same fundamentals as production rule systems, decision trees accept a series of inputs such as the game environment and ask questions in the same if-then style (\cite{tozour2002evolution}). Decision trees are branching structures containing nodes, where each node represents a test condition that leads either a sub-tree or a leaf, and the leaves are dead-end nodes that represent an action the AI should take (\cite{nareyek2004ai}, p.62). Traversal of a decision tree begins at the root node, where each conditional statement selects which child node to investigate next (\cite{nareyek2004ai}). This process repeats until a leaf node is reached containing the action the AI should take. This process is very easy to understand and implement (\cite{millington2019ai}, p.295), and the branching structure makes visualisation of the process more intuitive than the basic list used in a production rule system. Because of this, many consider decision trees to be one of, if not the simplest techiques to making AI (\cite{millington2019ai}, p.295; \cite{tozour2002evolution}, p.7).

FSMs, or Finite State Machines, are the most common approach to game AI (\cite{millington2019ai}, p309) due to the balance between their ease of understanding and the efficacy of their output. \citeauthor{orkin2006three} (\citeyear{orkin2006three}, p.1), said "If the audience of the Game Developers Conference were to be polled on the most common A.I techniques applied to games, one of the top answers would be Finite State Machines". FSMs consist of a directed graph where each node represents a state and the edges represent the transitions between them (\cite{tozour2002evolution}, p.6). What the states represent depend on the game, but they usually represent an expected behaviour of the character. A character can only be in one state at time and the state determines how they act at any given moment as well as the conditions necessary to switch to a different state (\cite{diller2004behavior}, p.3).

When using FSMs, controlling character behaviour can become quite straightforward as long as there is a clear idea about what is to be expected of a character and that these expectations of the character aren't too demanding. The process is still simple to implement like a decision tree due to the significant usage of rules that describe when the current state should switch to another (\cite{nareyek2004ai}, p.61), however, with a well designed FSM, the notion that a character's thought process is stateful gives off the impression that the character is indeed thinking. Given the correct game environment, this impression could appear to be good enough to compete with a neural network at a fraction of the time and resource cost despite not always being arriving at the optimal solution to the character's situation (\cite{sweetser2002current}).

Standard FSMs can only be in a single state at a time with no memory of the states it has previously been in, and the conditions that trigger the changing of state are described in the state itself (\cite{colledanchise2014performance}). With FSMs being such a good tool for the development of game AI, lots of implementations and additions have been discovered and used. One such addition has been the inclusion of fuzzy logic to make a FuSM (Fuzzy State Machine) (\cite{sweetser2002current}). The use of fuzzy logic involves replacing traditional boolean logic with real-numbers to allow a variable to represent something between $true$ and $false$ (\cite{tozour2002evolution}, p.7). With FuSMs, a character can partially be a member of multiple states at once - an enemy could be looking for a health pack while also fleeing from a player without there being a standalone state combining the two. This changes how transitions are implemented, as they are no longer instant or absolute. Instead, a state transition becomes an exchange of membership, where the FuSM's membership to a state increases while others decrease.

FuSMs are an improvement over FSMs because it allows you to combine and reuse states to have many variations that combine behaviour and rules of other states, however, this still means that whenever a new behaviour needs to be added to a character the programmer then needs to create a new state and the conditions of which this state integrates and transitions into other states. As states get added, these techniques becomes cumbersome to maintain (\cite{sweetser2002current}, p.2; \cite{lim2010evolving}, p.3). There's no easy way to combine the tests inside of FSMs, and even though FuSMs allow an AI to be a part of multiple states, selecting the conditions for a state transition is still very much a process that must be done by hand (\cite{millington2019ai}, p.313). 

The need for more flexibility in game AI has lead to the creation adaptation of modular algorithms such as behaviour trees (\cite{lim2010evolving}, p.1). Like decision trees, the recursive structure of a behaviour tree is simple to understand and implement while also being high level, allowing for more sophisticated AI to be created in a modular fashion through the use of subtrees (\cite{shoulson2011parameterizing}, p.144). A behaviour tree's nodes come in different types, but each type performs an action or check and then proceeds to succeed or fail (\cite{lim2010evolving}, p.4). It is these types that make the AI process at a higher level than standard decision trees. A selector node executes each child until one succeeds, in which case it itself will succed or otherwise fail. A sequence node does the opposite in the sense that it executes each child until one fails and will only succeed if every child also succeeds. A decorator node can only own and execute one child node and can do anything from repeat execution of the child or invert the child's returned status. When these node types are combined with leaf nodes that perform checks and actions to build trees, and then combined again to make trees containing subtrees, the simplicity and elegance of this technique certainly demonstrates why behaviour trees are getting attention (\cite{shoulson2011parameterizing}, p.144).

\subsubsection{AI and pathfinding}

One common requirement for game AI is for the characters to be able to traverse the areas of the game in a way which meets the player's expectations logically and efficiently - a task known as pathfinding. Regardless of what an AI decides to do, a pathfinding mechanic needs to be in place to allow the AI to navigate to where it needs to go, maneuvering around obstacles while still taking a sensible route (\cite{graham2003pathfinding}, p.60). For most games, the algorithm of choice is A* as it is the de-facto standard pathfinding algorithm (\cite{millington2019ai}, p.197; \cite{botea2004near}, p.2; \cite{nareyek2004ai}, p.64; \cite{leigh2007using}, p.73).

The A* algorithm analyses the game's map and generates a path from one location to another while minimising a $cost$ value - this value can represent anything but usually it represents the time or distance to travel along a given route (\cite{yap2002grid}, p.44). This means that the pathfinding algorithm itself doesn't decide where to go, only how to get there and the manner in which it does so. When asked to calculate a path, a pathfinding algorithm is provided a graph of nodes to determine which nodes can be reached from which (\cite{nareyek2004ai}, p.61). The algorithm isn't concerned with the what the data represents or in what form it is given, whether it is two-dimensional or three-dimensional, what the $cost$ is of travelling from one node to another, as long as it is equipped with the right functionality to digest this information (\cite{millington2019ai}, p.277; \cite{graham2003pathfinding}, p.60).

With the pathfinding process taking place after the decision has been made, the opportunity to involve the data gathered from pathfinding algorithm is missed. Often, the AI will decide to approach the nearest object, but obstacles in the way mean that the cost of navigating to the destination is greater than some alternative. While implementing the algorithm isn't difficult, ensuring the AI generates a path to the correct destination is difficult to do well (\cite{forbus2002qualitative}). Perhaps on the way to the destination, the character has to also navigate past traps or other hazards where it will need to decide whether to avoid or pass through - decisions that A* isn't fully equipped to deal with on it's own. If the AI wanted to factor in these hazards and real cost values, it would have to perform a more advanced check on where to travel too, maybe even using the pathfinding algorithm multiple times to definitely make sure that it wants to take the generated path, potentially ruining the game's performance.

Pathfinding algorithms are actually general purpose search algorithms applied to a spacial context (\cite{cui2011based}, p.125; \cite{orkin2003applying}, p.6; \cite{yap2002grid}, p.46). There is no such restriction that these algorithms should be restricted to pathfinding when in a games context. \citeauthor{millington2019ai} (\citeyear{millington2019ai}, p.197) said that "pathfinding can also be placed in the driving seat, making decisions about where to move as well as how to get there". With search algorithms having the flexibility of being able to traverse graphs of nodes representing any kind of data, it's no stretch to imagine the A* search algorithm being applied to a graph containing the same tests and actions found in decision and behaviour trees in order to generate a 'path' of actions rather than a path of spacial data (\cite{higgins2002generic}, p.114).

In this paper, the mechanisms of the A* search algorithm are examined and re-engineered, through the substitution of input and output types, to investigate the modularity and adaptability of an AI made in this way. The aim of using this approach is to bring decision-making and pathfinding closer together and therefore simplifying the overarching process of perceiving, deciding and interacting in the game world.

\subsection{Literature Review (11-12 pages)}

\subsubsection{Dijkstra's algorithm: A graph and tree search algorithm}

A search algorithm is a recursive method designed to find a match for a piece of data within a collection such as an array, graph or tree (\cite{friedman1976algorithm}). A piece of data is provided and the search algorithm typically returns whether it is present and it's location. (\citeauthor{dijkstra1959note})'s algorithm (\citeyear{dijkstra1959note}) is a search algorithm that operates on trees and graphs (which are then interpreted as trees). The algorithm calculates the shortest difference from any node on the graph to any other node. If a destination is provided, the algorithm can be terminated early to avoid unnecessary computation. 

Dijkstra's algorithm works through the recursive summation and comparison of distance values (\cite{dijkstra1959note}, p.269) - for each neighbour, the current node's distance from the start is added to the length between the current node and its neighbour. If this tentative value is lower than the current distance value of the neigbour, it replaces it. When all the neighbours have been considered, the node with the lowest tentative value on the graph is selected and permanently 'visited' (\cite{dijkstra1959note}). This process is repeated until the destination has been visited, and thus a path and the distance from the start to the destination can be retrieved. The problem with Dijkstra's algorithm is that it always selects the node with the lowest tentative distance value, meaning that the algorithm has no notion of direction and is calculating the lowest distance to nodes that may not be relevant to getting to the destination (\cite{millington2019ai}, p.214).

\subsubsection{A* algorithm: A heuristic best-first search algorithm}

A* is an improvement of Dijkstra's algorithm in this regard (\cite{hart1968formal}, p.101) - while it doesn't stray far from how Dijkstra's algorithm works in the sense that it operates using a tentative distance value and it keeps track of the nodes that have and haven't been visited, it does extend the algorithm using what's known as a heuristic approach (\cite{cui2011based}, p.126). "Heuristics are criteria, methods or principles for deciding which among several alternative courses of action promises to be the most effective in order to achieve some goal" (\cite{pearl1984heuristics}, p.3). This means that in a pathfinding situation, a heuristic function could estimate the distance to the goal, by ignoring walls and measuring in a straight line, to direct the algorithm in the right direction and avoid evaluating routes that travel in the wrong direction to make the process more efficient (\cite{cui2011based}, p.127). Heuristics enable A* to perform a best-first search (\cite{yap2002grid}, p46), as the heuristic now has the power to select which node is the best one to evaluate and prioritises it over the others (\cite{russell2016artificial}, p.94). A good heuristic algorithm should explore nodes that have the most potential for leading to the goal node (\cite{korf1985depth}).

When identifying the next node to expand, A* will take this heuristic distance into account using the formula $f(n) = g(n) + h(n)$ (\cite{hart1968formal}, p.102; \cite{russell2016artificial}, p.95), where $g(n)$ is the real distance the algorithm has calculated from the start node to node $n$, $h(n)$ is the heuristic distance from the current node $n$ and the destination, and $f(n)$ is the combination of these two metrics forming an estimate of the distance from the start node to the destination if travelling through route $n$ (\cite{hart1968formal}; \cite{millington2019ai}; \cite{graham2003pathfinding}, p.64). 

This heuristic component of A* transforms it into a family of algorithms where applying a different heuristic selects a different algorithm (\cite{hart1968formal}, p.107), moreover, implementing A* and using a heuristic that returns a constant value for all nodes reverts A* back into Dijkstra's algorithm (\cite{lester2005pathfinding}, p.10; \cite{millington2019ai}, p.237). Conversely, implementing a well-designed heuristic method can be used to guarantee optimal solutions, and using a heuristic that is somewhere in-between can output results with varying degrees of accuracy in exchange for faster exection (\cite{millington2019ai}, p.219).  The implementation of a good heuristic can be difficult, as making the heuristic take more factors into account for accuracy has the drawback of making the algorithm less efficient overall with the heuristic being frequently used throughout the process.

On the other hand, \citeauthor{graham2003pathfinding} (\citeyear{graham2003pathfinding}, p.68) argue that one of the constraints of games the industry is the "over-reliance" on the A* pathfinding algorithm and describe the development of its many extensions as a way of avoiding the discovery of new techniques. The pathfinding process can require a lot of CPU resources, sometimes to the point of stalling the game, when applied to larger graphs (\cite{cui2011based}, p.127; \cite{stentz1996map}, p.110; \cite{graham2003pathfinding}, p.67). This indicates that the performance of A* can vary depending on various factors, and is why it is important to optimise A* by selecting an suitable storage mechanism for the graph and internal node storage as well as using a reasonable heuristic that balances efficiency and efficacy (\cite{millington2019ai}, p.228).

\subsubsection{Decision making with A* using GOAP}

\citeauthor{orkin2003applying} (\citeyear{orkin2003applying}, p.11) expressed that expectations of AI are growing with the release of every new game, and that "we need to look toward more structured, formalized solutions to creating scalable, maintainable and re-usable decision making systems". Game AI techniques used in most games do not contain the scalability \citeauthor{orkin2003applying} envisions (\cite{laird2001human}, p.17), however, \citeauthor{higgins2002generic} (\citeyear{higgins2002generic}, p.117) declares that a pathfinding engine can be created generically, especially if you use a templated language to give the code even more reusability. Implementing A* in this way would allow it to be used for both regular pathfinding and any other uses you can get out of it(\cite{higgins2002generic}, p.120). With A* being efficient and optimisable (\cite{millington2019ai}, p.215), reusing A* for game AI has the potential to bring the benefits it usually brings to pathfinding while being adaptable enough to scale up and meet expectations.

Orkin (\citeyear{orkin2006three}, p.1) worked on the development of the game AI for the game F.E.A.R (\cite{FEAR}). The approach used for this AI was called GOAP which stands for 'Goal Oriented Action Planning' and actually uses the A* algorithm as part of its decision making process and "allows characters to decide not only what to do, but how to do it" (\cite{orkin2003applying}, p.1). GOAP changes the data to be processed by A* from spacial data such as coordinates into character AI state data, such as what you'd find in FSMs. Therefore, the output of A* is no longer a sequence of movements but a sequence of actions also known as a plan (\cite{orkin2003applying}, p.2; \cite{tozour2002evolution}, p.6).

GOAP essentially takes the idea of having state machines to encapsulate behaviours and replaces the hand-programming the conditions and connections of state transitions with a pathfinding process with the aim being the decoupling of states and their transitions (\cite{orkin2003applying}, p.2). The pathfinding process uses each node to represent a state and the edges between each node as the actions that lead to those states (\cite{orkin2003applying}, p.7) When GOAP is used, a goal is given for the character to achieve which gives the AI some direction and what is returned is the sequence of actions that will satisfy the goal (\cite{orkin2003applying}, p.1). An action is a representation of one thing the character will do to change the world in some way, like opening a door or picking up a weapon (\cite{orkin2003applying}, p.2); some actions have preconditions that require the execution of another action prior to it (\cite{orkin2003applying}, p.5). A* will then find the sequence of actions, the plan, that satisfies the character's goal while minimising an arbitrary $cost$ value - \citeauthor{orkin2003applying} (\citeyear{orkin2003applying}, p.4 - p.5) suggests that this process creates interesting character AI that can adapt to change while also having a code structure that is reusable, maintainable and "elegant".

\begin{figure}[h]
  \includegraphics[width=\linewidth]{img/goap_figure_1.png}
  \caption{An example of GOAP (\cite{orkin2003applying}, p.3)}
  \label{fig:goap}
\end{figure}

A* being used in GOAP means that fundamental formula $f(n) = g(n) + h(n)$ needs to be implemented: the calculation of a node's $fitness$ to determine the $cost$ of the actions taken up to that point and a heuristic function to determine the $cost$ of getting to the goal from a given node (\cite{orkin2003applying}, p.7). \citeauthor{orkin2003applying} (\citeyear{orkin2003applying}, p.7) also shares that the heuristic in GOAP can be calculated as the summation of unmet conditions of the goal node, but this is rather unclear. Given the example in figure \ref{fig:goap}, "the goal is to kill an enemy" (\cite{orkin2003applying}, p.3) could be interpreted in various ways that all seem unsuitable. If the condition is that the enemy is dead, the heuristic would not change for any action other than the goal state, making it potentially wasteful like Dijkstra's algorithm (\cite{millington2019ai}, p.214). Alternatively, if the condition was that the weapon needed to be drawn and ready to fire, it implies either the algorithm has been ran previously to determine the requirements of killing an enemy, or that they were programmed by the programmer - how would the starting state know that the weapon needed to be reloaded before drawing it? While GOAP does seem elegant, the graph in figure \ref{fig:goap} doesn't have a sizeable number of nodes; using dijkstra's algorithm wouldn't incur much of a performance cost with a graph this size. \citeauthor{orkin2003applying} (\citeyear{orkin2003applying}) suggests that a regressive search by starting at the goal and pathing to the start makes sense, and in figure \ref{fig:goap}, it does, as only one action can attack and so it's clear that the decision making process boils down to the character attacking the enemy.

Another problem of GOAP is that the high-level nature of the approach may not allow for enough control of the game (\cite{stanciu2012implementing}, p.87) - while making abstractions of the world is necessary for AI to process the information (\cite{buro2004call}, p.2) and thinking on a higher level is beneficial for making more humanlike decisions, the precision of the actions taken by the AI is both low level and very noticeable if incorrect (\cite{graham2003pathfinding}, p.60). Figure \ref{fig:goap} shows a decision making process that decides what to do but not necessarily where to do it. When the action "Goto Point" in figure \ref{fig:goap} get executed, there's no indication of how the destinations are generated other than satisfying the preconditions for another action (for example, moving to an object to activate it) (\cite{orkin2003applying}, p.7). 

This isn't specific to GOAP as it applies to every AI approach where the generation of these locations isn't part of the AI. Is the character's AI deciding where to go, or just deciding that it has to go somewhere? In order for "Goto Cover" to act in a similar way to the "Goto Point" action, a goal or action precondition would either have to require the character to be out of line-of-sight, or be using one of the designated cover spots. The latter of these would require the designation of these cover spots and then A* would be employed to find which coverspot would be the quickest to navigate to. The former would require calculating the closest position that allows the escape of line-of-sight. Both of these would have their own problems, and wrapping the entire pathfinding process as a single action would mean that the information from the pathfinding request would not be used in the decision making at all. Without utilising the AI's ability to create an impression of thought, the information given to these actions could be poorly chosen and "may be perceived as a lack of intelligence by a human player" (\cite{graham2003pathfinding}, p.63). To circumvent this, multiple "Goto Point" actions would be needed so that the planning system can evaluate each point into the workflow, but this could introduce its own problems without a well-designed heuristic that can distinguish between the different locations.

END

A solution is an action sequence, so search algorithms work by considering various possible action sequences (Artificial Intelligence: A modern approach). These pathfinding algorithms can search for a solution for traveling from A to B, but could potentially do more with the redefinition of what an action is. In theory, you could apply 'best first' to game interactions in a scenario.

Normally, pathfinding takes a backseat when it comes to decision making and just generates the requested path. However, since pathfinding algorithms are just search algorithms being applied to graphs, there's no restriction that these nodes and edges have to correspond to locations and distances.

Dijkstra's algorithm is a simple pathfinding algorithm that finds the shortest route to every node in the graph. It's an uninformed algorithm.

AStar is the defacto algorithm for pathfinding. Its actually a family of algorithms where the heuristic dictates how it functions. AStar with no heuristic is dijkstras, and could also be used to become a best-first, breadth first and depth-first.

Changing how A* evaluates actions and applies a heuristic will allow it to operate differently. A good heuristic will allow the algorithm to discard 'bad moves' (Depth-first iterative-deepening: an optimal admissable tree search).

A* becomes inefficient on bigger maps and lots of techniques have been created to adjust to these problems. Having lots of edges per node could make A* very inefficient.

This paper will investigate the effects of different cost and heuristic functions when changing the types involved with A* to see how agents perform.

\subsection{Methods and Methodologies}

\section {Design, Development and Evaluation}

\section{Design (14-15 pages when combined with development)}

\subsection {Development (14-15 pages when combined with design)}

\subsection {Results and Evaluation (11-12 pages including critical review)}

\section {Conclusions}

\subsection {Conclusions and Critical Review}

\printbibliography
\end{document}
